folder name: client.py



"""
Ollama Client

Wrapper around Ollama's HTTP API with:
- Connection pooling
- Retry logic
- Streaming support
- Error handling
"""

from typing import List, Dict, Any, Optional, AsyncIterator, Union
from dataclasses import dataclass
import json
import asyncio

import httpx
import structlog

logger = structlog.get_logger()


@dataclass
class ChatMessage:
    """Chat message format."""
    role: str  # system, user, assistant, tool
    content: str
    images: Optional[List[str]] = None
    tool_calls: Optional[List[Dict]] = None


@dataclass
class GenerationResponse:
    """Response from generate endpoint."""
    response: str
    done: bool
    context: Optional[List[int]] = None
    total_duration: Optional[int] = None
    load_duration: Optional[int] = None
    prompt_eval_count: Optional[int] = None
    eval_count: Optional[int] = None


@dataclass
class ChatResponse:
    """Response from chat endpoint."""
    message: ChatMessage
    done: bool
    total_duration: Optional[int] = None
    load_duration: Optional[int] = None
    prompt_eval_count: Optional[int] = None
    eval_count: Optional[int] = None


class OllamaClient:
    """
    Client for Ollama HTTP API.
    
    Endpoints:
    - POST /api/generate - Generate completion
    - POST /api/chat - Chat completion
    - POST /api/embed - Generate embeddings
    - GET /api/tags - List models
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        timeout: float = 120.0,
        max_retries: int = 3
    ):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.max_retries = max_retries
        
        # HTTP client with connection pooling
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(timeout),
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            http2=True
        )
        
        self.logger = structlog.get_logger()
    
    async def generate(
        self,
        model: str,
        prompt: str,
        system: Optional[str] = None,
        template: Optional[str] = None,
        context: Optional[List[int]] = None,
        stream: bool = False,
        raw: bool = False,
        format: Optional[str] = None,  # json or JSON schema
        images: Optional[List[str]] = None,
        options: Optional[Dict[str, Any]] = None,
        keep_alive: Optional[str] = "5m"
    ) -> Union[str, AsyncIterator[str]]:
        """
        Generate a completion.
        
        Args:
            model: Model name (e.g., "llama3.2:3b")
            prompt: Prompt text
            system: System message
            template: Custom prompt template
            context: Conversation context from previous call
            stream: Whether to stream response
            raw: Whether to use raw prompt (no template)
            format: Response format (json or schema)
            images: Base64-encoded images for multimodal
            options: Model parameters (temperature, etc.)
            keep_alive: How long to keep model loaded
            
        Returns:
            Complete response string, or async iterator if streaming
        """
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        if system:
            payload["system"] = system
        if template:
            payload["template"] = template
        if context:
            payload["context"] = context
        if raw:
            payload["raw"] = raw
        if format:
            payload["format"] = format
        if images:
            payload["images"] = images
        if options:
            payload["options"] = options
        if keep_alive:
            payload["keep_alive"] = keep_alive
        
        if stream:
            return self._generate_stream(url, payload)
        
        # Non-streaming
        response_text = ""
        for attempt in range(self.max_retries):
            try:
                response = await self.client.post(url, json=payload)
                response.raise_for_status()
                
                data = response.json()
                return data.get("response", "")
                
            except httpx.HTTPError as e:
                self.logger.error("generate_request_failed", 
                                attempt=attempt, error=str(e))
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
        
        return response_text
    
    async def _generate_stream(
        self, 
        url: str, 
        payload: Dict
    ) -> AsyncIterator[str]:
        """
        Stream generate response.
        
        Args:
            url: API endpoint
            payload: Request payload
            
        Yields:
            Response chunks
        """
        async with self.client.stream("POST", url, json=payload) as response:
            response.raise_for_status()
            
            async for line in response.aiter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        chunk = data.get("response", "")
                        if chunk:
                            yield chunk
                        
                        if data.get("done"):
                            break
                    except json.JSONDecodeError:
                        continue
    
    async def chat(
        self,
        model: str,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        format: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None,
        keep_alive: Optional[str] = "5m"
    ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:
        """
        Chat completion with message history.
        
        Args:
            model: Model name
            messages: List of message dicts with role and content
            tools: List of tool definitions for function calling
            stream: Whether to stream response
            format: Response format
            options: Model parameters
            keep_alive: How long to keep model loaded
            
        Returns:
            ChatResponse or async iterator
        """
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        if tools:
            payload["tools"] = tools
        if format:
            payload["format"] = format
        if options:
            payload["options"] = options
        if keep_alive:
            payload["keep_alive"] = keep_alive
        
        if stream:
            return self._chat_stream(url, payload)
        
        # Non-streaming
        for attempt in range(self.max_retries):
            try:
                response = await self.client.post(url, json=payload)
                response.raise_for_status()
                
                data = response.json()
                return ChatResponse(
                    message=ChatMessage(**data.get("message", {})),
                    done=data.get("done", True),
                    total_duration=data.get("total_duration"),
                    load_duration=data.get("load_duration"),
                    prompt_eval_count=data.get("prompt_eval_count"),
                    eval_count=data.get("eval_count")
                )
                
            except httpx.HTTPError as e:
                self.logger.error("chat_request_failed", 
                                attempt=attempt, error=str(e))
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
        
        raise Exception("Max retries exceeded")
    
    async def _chat_stream(
        self, 
        url: str, 
        payload: Dict
    ) -> AsyncIterator[str]:
        """
        Stream chat response.
        
        Args:
            url: API endpoint
            payload: Request payload
            
        Yields:
            Response content chunks
        """
        async with self.client.stream("POST", url, json=payload) as response:
            response.raise_for_status()
            
            async for line in response.aiter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        message = data.get("message", {})
                        content = message.get("content", "")
                        if content:
                            yield content
                        
                        if data.get("done"):
                            break
                    except json.JSONDecodeError:
                        continue
    
    async def embed(
        self,
        model: str,
        input: Union[str, List[str]],
        options: Optional[Dict[str, Any]] = None,
        keep_alive: Optional[str] = "5m"
    ) -> List[List[float]]:
        """
        Generate embeddings.
        
        Args:
            model: Embedding model name
            input: Text or list of texts to embed
            options: Model parameters
            keep_alive: How long to keep model loaded
            
        Returns:
            List of embedding vectors
        """
        url = f"{self.base_url}/api/embed"
        
        payload = {
            "model": model,
            "input": input if isinstance(input, list) else [input]
        }
        
        if options:
            payload["options"] = options
        if keep_alive:
            payload["keep_alive"] = keep_alive
        
        response = await self.client.post(url, json=payload)
        response.raise_for_status()
        
        data = response.json()
        return data.get("embeddings", [])
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """
        List available models.
        
        Returns:
            List of model metadata
        """
        url = f"{self.base_url}/api/tags"
        
        response = await self.client.get(url)
        response.raise_for_status()
        
        data = response.json()
        return data.get("models", [])
    
    async def pull_model(
        self, 
        name: str, 
        stream: bool = False
    ) -> Union[Dict, AsyncIterator[Dict]]:
        """
        Pull a model from the registry.
        
        Args:
            name: Model name to pull
            stream: Whether to stream progress
            
        Returns:
            Pull result or progress stream
        """
        url = f"{self.base_url}/api/pull"
        payload = {"name": name, "stream": stream}
        
        if stream:
            async def progress_stream():
                async with self.client.stream("POST", url, json=payload) as response:
                    async for line in response.aiter_lines():
                        if line:
                            yield json.loads(line)
            return progress_stream()
        
        response = await self.client.post(url, json=payload)
        response.raise_for_status()
        return response.json()
    
    async def delete_model(self, name: str) -> Dict:
        """
        Delete a model.
        
        Args:
            name: Model name to delete
            
        Returns:
            Delete result
        """
        url = f"{self.base_url}/api/delete"
        payload = {"name": name}
        
        response = await self.client.delete(url, json=payload)
        response.raise_for_status()
        return {"status": "deleted", "model": name}
    
    async def check_connection(self) -> bool:
        """
        Check if Ollama server is reachable.
        
        Returns:
            True if connected
        """
        try:
            response = await self.client.get(f"{self.base_url}/api/tags", timeout=5.0)
            return response.status_code == 200
        except Exception:
            return False
    
    async def close(self):
        """Close HTTP client."""
        await self.client.aclose()
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()






>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
file: interface/cli.py



#!/usr/bin/env python3
"""
Command-Line Interface for DevOps AI Agent

Provides an interactive CLI for interacting with the agent.
"""

import asyncio
import sys
from pathlib import Path
from typing import Optional

import click
import structlog
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.syntax import Syntax
from rich.spinner import Spinner
from rich.live import Live

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from agent.core import DevOpsAgent, AgentConfig
from llm.client import OllamaClient
from tools.base import tool_registry
from tools.docker.tool import DockerTool
from security.command_filter import CommandFilter

console = Console()
logger = structlog.get_logger()


@click.group()
@click.option('--verbose', '-v', is_flag=True, help='Enable verbose output')
@click.option('--model', '-m', default='qwen2.5:7b', help='Model to use')
@click.option('--config', '-c', type=click.Path(), help='Config file path')
@click.pass_context
def cli(ctx, verbose, model, config):
    """DevOps AI Agent - Your local DevOps assistant."""
    ctx.ensure_object(dict)
    ctx.obj['verbose'] = verbose
    ctx.obj['model'] = model
    ctx.obj['config'] = config
    
    # Setup logging
    if verbose:
        structlog.configure(
            wrapper_class=structlog.make_filtering_bound_logger(10)
        )


@cli.command()
@click.pass_context
def chat(ctx):
    """Start interactive chat session."""
    model = ctx.obj['model']
    
    console.print(Panel.fit(
        f"[bold blue]DevOps AI Agent[/bold blue]\\n"
        f"Model: [green]{model}[/green] | "
        f"Type [yellow]'exit'[/yellow] or [yellow]'quit'[/yellow] to exit",
        title="Welcome",
        border_style="blue"
    ))
    
    # Check Ollama connection
    asyncio.run(_check_ollama())
    
    # Initialize agent
    agent = asyncio.run(_init_agent(model))
    
    # Chat loop
    while True:
        try:
            user_input = console.input("\\n[bold green]You:[/bold green] ")
            
            if user_input.lower() in ['exit', 'quit', 'q']:
                console.print("[yellow]Goodbye![/yellow]")
                break
            
            if user_input.lower() == 'clear':
                agent.clear_context()
                console.print("[dim]Context cleared.[/dim]")
                continue
            
            if user_input.lower().startswith('/'):
                _handle_command(user_input, agent)
                continue
            
            # Process input
            console.print("[bold blue]Agent:[/bold blue] ", end="")
            
            response_chunks = []
            async for chunk in agent.chat(user_input):
                console.print(chunk, end="")
                response_chunks.append(chunk)
            
            console.print()  # Newline after response
            
        except KeyboardInterrupt:
            console.print("\\n[yellow]Interrupted. Type 'exit' to quit.[/yellow]")
        except EOFError:
            break


@cli.command()
@click.argument('task')
@click.option('--dry-run', is_flag=True, help='Show what would be done')
@click.pass_context
def run(ctx, task, dry_run):
    """Execute a single task."""
    model = ctx.obj['model']
    
    console.print(f"[dim]Task: {task}[/dim]")
    console.print(f"[dim]Model: {model}[/dim]\\n")
    
    if dry_run:
        console.print("[yellow]Dry run mode - no commands will be executed[/yellow]\\n")
    
    agent = asyncio.run(_init_agent(model))
    
    console.print("[bold blue]Agent:[/bold blue] ", end="")
    
    async def process():
        async for chunk in agent.chat(task):
            console.print(chunk, end="")
    
    asyncio.run(process())
    console.print()


@cli.command()
def tools():
    """List available tools."""
    console.print("[bold]Available Tools:[/bold]\\n")
    
    # Import all tools to register them
    from tools.docker.tool import DockerTool
    
    all_tools = tool_registry.list_all()
    
    for tool_info in all_tools:
        status = "[green]✓[/green]" if tool_info['available'] else "[red]✗[/red]"
        console.print(f"{status} [bold]{tool_info['name']}[/bold]")
        console.print(f"  {tool_info['description']}")
        
        if tool_info['operations']:
            console.print("  Operations:")
            for op in tool_info['operations'][:5]:  # Show first 5
                risk = op.get('risk_level', 'safe')
                risk_color = {
                    'safe': 'green',
                    'read_only': 'blue',
                    'approval': 'yellow',
                    'blocked': 'red'
                }.get(risk, 'white')
                console.print(f"    • {op['name']} ([{risk_color}]{risk}[/{risk_color}])")
        
        console.print()


@cli.command()
def models():
    """List available Ollama models."""
    asyncio.run(_list_models())


async def _check_ollama():
    """Check Ollama connection."""
    async with OllamaClient() as client:
        if await client.check_connection():
            console.print("[dim]Connected to Ollama[/dim]\\n")
        else:
            console.print("[red]Error: Cannot connect to Ollama.[/red]")
            console.print("[dim]Make sure Ollama is running on localhost:11434[/dim]")
            sys.exit(1)


async def _init_agent(model: str) -> DevOpsAgent:
    """Initialize the agent."""
    llm_client = OllamaClient()
    
    # Load security config
    config_path = Path(__file__).parent.parent / "config" / "security.yaml"
    command_filter = CommandFilter(str(config_path) if config_path.exists() else None)
    
    # Register tools
    docker_tool = DockerTool()
    tool_registry.register(docker_tool)
    
    agent_config = AgentConfig(model=model)
    
    return DevOpsAgent(
        llm_client=llm_client,
        tool_registry=tool_registry,
        command_filter=command_filter,
        config=agent_config
    )


def _handle_command(cmd: str, agent: DevOpsAgent):
    """Handle special commands."""
    if cmd == '/status':
        state = agent.get_state()
        console.print(f"[dim]Agent state: {state.value}[/dim]")
    
    elif cmd == '/context':
        history = agent.get_conversation_history()
        console.print(f"[dim]Messages in context: {len(history)}[/dim]")
    
    elif cmd == '/help':
        console.print("""
[bold]Special Commands:[/bold]
  /status    - Show agent status
  /context   - Show context info
  /help      - Show this help
  clear      - Clear conversation context
  exit       - Exit the chat
        """)
    
    else:
        console.print(f"[yellow]Unknown command: {cmd}[/yellow]")


async def _list_models():
    """List available models."""
    async with OllamaClient() as client:
        try:
            models = await client.list_models()
            
            console.print("[bold]Available Models:[/bold]\\n")
            
            for model in models:
                name = model.get('name', 'unknown')
                size = model.get('size', 0)
                size_str = _format_size(size)
                
                console.print(f"• [bold]{name}[/bold] ({size_str})")
                
        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")


def _format_size(size_bytes: int) -> str:
    """Format byte size to human readable."""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} PB"


if __name__ == '__main__':
    cli()

